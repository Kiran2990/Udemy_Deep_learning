{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network - Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dataset of images. These were retrieved from [SuperDataScience](https://www.superdatascience.com/deep-learning/) website. These are images of cats and dogs. We will build a Convolutional Neural Network which can classify these images. We need to pre-process these images before sending them as input to our network. But instead of doing this manually, we will be using Keras inbuilt pre-processing tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 10,000 images of which 5000 are dogs and 5000 are cats. We are going to use 4000 dogs and cats in the training set and 1000 dogs and cats in the test set. Now for Keras to understand the different images and to distiguish between training set and test set, we need to create a specific folder structure as below\n",
    "    * dataset/\n",
    "        * training_set/\n",
    "            * dogs/\n",
    "            * cats/\n",
    "        * test_set/\n",
    "            * dogs/\n",
    "            * cats/\n",
    "\n",
    "Using this structure, the Keras library can easily label the data as cats and dogs and check for classification errors during the training phase of the algorithm. Now we are ready to build the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 4 steps in CNN. Convolution --> Pooling --> Flatten --> Full Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the first convolution layer using Convolution2D function. If we were using video's we would need to use 3D as we also have the time parameter. \n",
    "\n",
    "To this layer we will specify the number of feature detectors. We will use 32 feature detectors with a dimension of 3X3 for each filter. This will be the first 3 arguments to the function. \n",
    "\n",
    "Next we need to specify the input_shape. We are going to use 64X64 colored images. Since all the images are not of this dimension we will force this in the image preprocessing section. After which the dimensions of the input_shape will be 64X64X3. \n",
    "\n",
    "The activation function we will be using is 'relu' which will help us to enforce non-linearity in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will perform Max Pooling on images. We will be using a pool size of 2X2. This is done to reduce the number of nodes to the next layer and also pooling detects the features regardless of its orientation introducing spatial variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to perform flattening. This simple process is unwrapping the matrix into a single vector which can be inputted to our fully connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be adding a second convolutional layer to create a deeper network and also to improve the accuracy of our CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to add a fully connected layer. Since we have flattened the data, we can now think of the data as independent variables which can be fed to an ANN. We will have 2 layers, 1 hidden layer and 1 ouput layer. The hidden layer will have 128 nodes (This is just a random choice, no rule of thumb) and output layer will have just 1 node as we are using binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all our layers are created, we need to compile the network. In this process we will be fixing the parameters of the netwrok.\n",
    "\n",
    "optimizer - Algorithm we want to use to find the optimal set of weight for the network. We will be using stochastic gradient descent for this problem. 'adam' is the most common SGD algorithm.\n",
    "\n",
    "loss - We need to fix a loss function within the SGD algorithm. We wil be 'binary_crossentropy' which is similar to the loss funciton in logistic regression model. If we have more than 2 categories in the output, we need to use 'categorical_crossentropy'.\n",
    "\n",
    "metrics - A measure to find the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our network is ready. We can send in images to be classified. But as discussed earlier, we need to pre-process the images before sending them to the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the images to the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be performing some image augmentation before training. This is done to prevent overfitting. Also, we have just 8000 images to train the network. This is not a lot of images and we might not get good accuracy. Keras will create multiple batches of the image and to each batch it will apply some random transformation to the image such as rotating, flipping, shifting, sheering etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the training and the test sets, we can train our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 8000,\n",
    "                         epochs = 25,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Epoch 1/25\n",
    "250/250 [==============================] - 304s 1s/step - loss: 0.6511 - acc: 0.6118 - val_loss: 0.5887 - val_acc: 0.6768\n",
    "* Epoch 2/25\n",
    "250/250 [==============================] - 308s 1s/step - loss: 0.5861 - acc: 0.6944 - val_loss: 0.6040 - val_acc: 0.6763\n",
    "* Epoch 3/25\n",
    "250/250 [==============================] - 324s 1s/step - loss: 0.5505 - acc: 0.7208 - val_loss: 0.5236 - val_acc: 0.7464\n",
    "* Epoch 4/25\n",
    "250/250 [==============================] - 334s 1s/step - loss: 0.5196 - acc: 0.7416 - val_loss: 0.5326 - val_acc: 0.7379\n",
    "* Epoch 5/25\n",
    "250/250 [==============================] - 306s 1s/step - loss: 0.4887 - acc: 0.7584 - val_loss: 0.4805 - val_acc: 0.7680\n",
    "* Epoch 6/25\n",
    "250/250 [==============================] - 308s 1s/step - loss: 0.4873 - acc: 0.7649 - val_loss: 0.4675 - val_acc: 0.7785\n",
    "* Epoch 7/25\n",
    "250/250 [==============================] - 300s 1s/step - loss: 0.4643 - acc: 0.7719 - val_loss: 0.4769 - val_acc: 0.7799\n",
    "* Epoch 8/25\n",
    "250/250 [==============================] - 300s 1s/step - loss: 0.4445 - acc: 0.7925 - val_loss: 0.4559 - val_acc: 0.7927\n",
    "* Epoch 9/25\n",
    "250/250 [==============================] - 301s 1s/step - loss: 0.4401 - acc: 0.7959 - val_loss: 0.4388 - val_acc: 0.7954\n",
    "* Epoch 10/25\n",
    "250/250 [==============================] - 299s 1s/step - loss: 0.4221 - acc: 0.8019 - val_loss: 0.4669 - val_acc: 0.7800\n",
    "* Epoch 11/25\n",
    "250/250 [==============================] - 324s 1s/step - loss: 0.4134 - acc: 0.8114 - val_loss: 0.4567 - val_acc: 0.7889\n",
    "* Epoch 12/25\n",
    "250/250 [==============================] - 469s 2s/step - loss: 0.3959 - acc: 0.8195 - val_loss: 0.4568 - val_acc: 0.7869\n",
    "* Epoch 13/25\n",
    "250/250 [==============================] - 371s 1s/step - loss: 0.3915 - acc: 0.8196 - val_loss: 0.4405 - val_acc: 0.7969\n",
    "* Epoch 14/25\n",
    "250/250 [==============================] - 395s 2s/step - loss: 0.3840 - acc: 0.8283 - val_loss: 0.4531 - val_acc: 0.7869\n",
    "* Epoch 15/25\n",
    "250/250 [==============================] - 331s 1s/step - loss: 0.3700 - acc: 0.8320 - val_loss: 0.4411 - val_acc: 0.8131\n",
    "* Epoch 16/25\n",
    "250/250 [==============================] - 283s 1s/step - loss: 0.3695 - acc: 0.8269 - val_loss: 0.4332 - val_acc: 0.8119\n",
    "* Epoch 17/25\n",
    "250/250 [==============================] - 387s 2s/step - loss: 0.3518 - acc: 0.8420 - val_loss: 0.4239 - val_acc: 0.8100\n",
    "* Epoch 18/25\n",
    "250/250 [==============================] - 362s 1s/step - loss: 0.3428 - acc: 0.8442 - val_loss: 0.4261 - val_acc: 0.8150\n",
    "* Epoch 19/25\n",
    "250/250 [==============================] - 390s 2s/step - loss: 0.3254 - acc: 0.8596 - val_loss: 0.4212 - val_acc: 0.8255\n",
    "* Epoch 20/25\n",
    "250/250 [==============================] - 399s 2s/step - loss: 0.3150 - acc: 0.8644 - val_loss: 0.4395 - val_acc: 0.8154\n",
    "* Epoch 21/25\n",
    "250/250 [==============================] - 339s 1s/step - loss: 0.3089 - acc: 0.8635 - val_loss: 0.4380 - val_acc: 0.8180\n",
    "* Epoch 22/25\n",
    "250/250 [==============================] - 361s 1s/step - loss: 0.3020 - acc: 0.8679 - val_loss: 0.4338 - val_acc: 0.8131\n",
    "* Epoch 23/25\n",
    "250/250 [==============================] - 323s 1s/step - loss: 0.2916 - acc: 0.8760 - val_loss: 0.4246 - val_acc: 0.8195\n",
    "* Epoch 24/25\n",
    "250/250 [==============================] - 308s 1s/step - loss: 0.2846 - acc: 0.8785 - val_loss: 0.4684 - val_acc: 0.8125\n",
    "* Epoch 25/25\n",
    "250/250 [==============================] - 303s 1s/step - loss: 0.2717 - acc: 0.8830 - val_loss: 0.4480 - val_acc: 0.8180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
