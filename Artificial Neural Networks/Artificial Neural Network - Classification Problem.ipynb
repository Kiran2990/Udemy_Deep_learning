{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network - Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Business Problem\n",
    "\n",
    "We have a dataset of a bank records. There are 10,000 customers with details such as name, credit score, age, tenure, balance, etc. It also has a column called 'Exited' which tells us if a customer has left the bank. The bank needs to look at this data and understand why it has a high churn rate ( many people exiting the bank ). We need to look at the dataset and determine which customer's are most likely to leave the bank.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "rows, columns = dataset.shape\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 3:columns-1].values\n",
    "y = dataset.iloc[:, columns-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Values\n",
      "[[619 'France' 'Female' 42 2 0.0 1 1 1 101348.88]\n",
      " [608 'Spain' 'Female' 41 1 83807.86 1 0 1 112542.58]\n",
      " [502 'France' 'Female' 42 8 159660.8 3 1 0 113931.57]\n",
      " [699 'France' 'Female' 39 1 0.0 2 0 0 93826.63]\n",
      " [850 'Spain' 'Female' 43 2 125510.82 1 1 1 79084.1]]\n",
      "\n",
      "Y Values\n",
      "[1 0 1 0 0 1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Checking the values of X and y\n",
    "print(\"X Values\")\n",
    "print(X[:5])\n",
    "print(\"\\nY Values\")\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "\n",
    "# Encoding the Country\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "# Encoding the Gender\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "\n",
    "# OneHotEncoder will be applied only for Country Column as we have 3 categories. For gender column we have only 2 \n",
    "# categories. We will also remove 1 column to avoid the dummy variable trap\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X = X[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.00', '1.00', '608.00', '0.00', '41.00', '1.00', '83807.86', '1.00', '0.00', '1.00', '112542.58']\n"
     ]
    }
   ],
   "source": [
    "# Viewing the input data after preprocessing\n",
    "print([\"{0:0.2f}\".format(i) for i in X[1,:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "# Module required to initalize the network\n",
    "from keras.models import Sequential\n",
    "# Module required to build the layers of the network\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the ANN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our network intialized we will start building the layers of the network. First we create an input layer and the first hidden layer. We need to figure out how many nodes we want in the 1st hidden layer. As there is no rule of thumb, we can use parameter tuning to determine this for more complicated networks. But for now we will take the average of the input and output nodes which is 6. Our input will have 11 nodes as we have 11 independent variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(units=6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using 'uniform' function to initalize the weights for this layer. This will help us to get a random set of weights which are uniformly distributed and close to 0.  We will be using the rectifier activation function for this layer using 'relu' which returns max(x,0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units=6, kernel_initializer = 'uniform', activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This above layer is not typically necessary for this problem because we can use a simple network to solve the problem. But adding it not going to cause much harm and we are trying to build a deep neural network. Hence we will add 1 with the same node as the 1st hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to add the ouput layer. The output layer will have just 1 node as we need to classify the data into 1 or 0 which is a binary outcome. We will be using 'sigmoid' activation function. If we had more than 2 categories we can use 'softmax' activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all our layers are created, we need to compile the network. In this process we will be fixing the parameters of the netwrok.\n",
    "\n",
    "optimizer - Algorithm we want to use to find the optimal set of weight for the network. We will be using stochastic gradient descent for this problem. 'adam' is the most common SGD algorithm.\n",
    "\n",
    "loss - We need to fix a loss function within the SGD algorithm. We wil be 'binary_crossentropy' which is similar to the loss funciton in logistic regression model. If we have more than 2 categories in the output, we need to use 'categorical_crossentropy'.\n",
    "\n",
    "metrics - A measure to find the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to fit the ANN to our training set. We need to input the X_train and y_train variables. We also need to input the number of epochs in which our training data will be passed through the network. And we also need to specify after how many training data inputs, we want to update the weight. This is specefied using batch_size argument. Again there is no rule of thumb for choosing these values. We will just set it to 100 epochs and batch_size of 10 which means after every 10 training data rows, our weights will be updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 2s 257us/step - loss: 0.4906 - acc: 0.7955\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 0.4293 - acc: 0.7960\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 1s 172us/step - loss: 0.4252 - acc: 0.7960\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 1s 183us/step - loss: 0.4213 - acc: 0.8047\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.4186 - acc: 0.8236\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 0.4165 - acc: 0.8254\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.4147 - acc: 0.8292\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 1s 182us/step - loss: 0.4134 - acc: 0.8310\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 0.4118 - acc: 0.8324\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 2s 189us/step - loss: 0.4107 - acc: 0.8339\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 0.4095 - acc: 0.8344\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.4087 - acc: 0.8329\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 1s 170us/step - loss: 0.4082 - acc: 0.8359\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 0.4076 - acc: 0.8355\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 1s 167us/step - loss: 0.4065 - acc: 0.8347\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 1s 173us/step - loss: 0.4068 - acc: 0.8351\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 0.4059 - acc: 0.8349\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.4054 - acc: 0.8344\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 0.4052 - acc: 0.8339\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 2s 189us/step - loss: 0.4047 - acc: 0.8350\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 0.4046 - acc: 0.8337\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 1s 170us/step - loss: 0.4043 - acc: 0.8354\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 1s 187us/step - loss: 0.4032 - acc: 0.8347\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 2s 189us/step - loss: 0.4037 - acc: 0.8346\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 0.4038 - acc: 0.8356\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 2s 237us/step - loss: 0.4032 - acc: 0.8354\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 2s 250us/step - loss: 0.4030 - acc: 0.8342\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 0.4031 - acc: 0.8342\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 2s 235us/step - loss: 0.4032 - acc: 0.8349\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 0.4025 - acc: 0.8355\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.4023 - acc: 0.8349\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 0.4022 - acc: 0.8344\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 0.4027 - acc: 0.8344\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 0.4023 - acc: 0.8339\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 1s 187us/step - loss: 0.4027 - acc: 0.8346\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 1s 184us/step - loss: 0.4019 - acc: 0.8350\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.4019 - acc: 0.8334\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 0.4021 - acc: 0.8349\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 1s 183us/step - loss: 0.4020 - acc: 0.8361\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 0.4018 - acc: 0.8334\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 0.4021 - acc: 0.8327\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 0.4015 - acc: 0.8354\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 1s 182us/step - loss: 0.4016 - acc: 0.8345\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 0.4008 - acc: 0.8347\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 0.4016 - acc: 0.8345\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 0.4018 - acc: 0.8324\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.4011 - acc: 0.8355\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 0.4008 - acc: 0.8351\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 1s 170us/step - loss: 0.4014 - acc: 0.8342\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 1s 166us/step - loss: 0.4009 - acc: 0.8336\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 0.4011 - acc: 0.8347\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 1s 152us/step - loss: 0.4010 - acc: 0.8344\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 0.4010 - acc: 0.8346\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 0.4008 - acc: 0.8359\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 2s 263us/step - loss: 0.4010 - acc: 0.8356\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 2s 241us/step - loss: 0.4009 - acc: 0.8364\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 0.4008 - acc: 0.8360\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 0.4007 - acc: 0.8350\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 2s 230us/step - loss: 0.4003 - acc: 0.8342\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 1s 183us/step - loss: 0.4010 - acc: 0.8356\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 1s 155us/step - loss: 0.3999 - acc: 0.8357\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.4008 - acc: 0.8332\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 0.4007 - acc: 0.8341\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.4006 - acc: 0.8332\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 0.4000 - acc: 0.8359\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 2s 188us/step - loss: 0.4006 - acc: 0.8346\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 1s 166us/step - loss: 0.4003 - acc: 0.8359\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 1s 151us/step - loss: 0.4004 - acc: 0.8330\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 0.4008 - acc: 0.8351\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 0.4003 - acc: 0.8350\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 0.4004 - acc: 0.8355\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 0.3999 - acc: 0.8332\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 0.4003 - acc: 0.8345\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 1s 151us/step - loss: 0.4005 - acc: 0.8350\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 0.4002 - acc: 0.8335\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.4003 - acc: 0.8357\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 1s 154us/step - loss: 0.4003 - acc: 0.8351\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 1s 150us/step - loss: 0.4002 - acc: 0.8354\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 1s 162us/step - loss: 0.4005 - acc: 0.8345\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 0.3998 - acc: 0.8336\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.4001 - acc: 0.8352\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 183us/step - loss: 0.4000 - acc: 0.8347\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 1s 150us/step - loss: 0.4002 - acc: 0.8357\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 1s 155us/step - loss: 0.3998 - acc: 0.8350\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 1s 149us/step - loss: 0.4001 - acc: 0.8350\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 1s 150us/step - loss: 0.4002 - acc: 0.8354\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 1s 151us/step - loss: 0.4001 - acc: 0.8356\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 1s 149us/step - loss: 0.3999 - acc: 0.8366\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.4004 - acc: 0.8352\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3999 - acc: 0.8347\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 0.3998 - acc: 0.8335\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 1s 173us/step - loss: 0.3996 - acc: 0.8356\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 0.4001 - acc: 0.8346\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 2s 306us/step - loss: 0.3998 - acc: 0.8354\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 0.3999 - acc: 0.8341\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 2s 298us/step - loss: 0.4001 - acc: 0.8362\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 0.3999 - acc: 0.8354\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 1s 163us/step - loss: 0.3999 - acc: 0.8351\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.3995 - acc: 0.8347\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 2s 189us/step - loss: 0.4002 - acc: 0.8336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a1db8e2e8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making the predictions and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# We get a probability as outpur because we have used a sigmoid activation function. \n",
    "# If we want a binary ouput we need to process the output a little more as below. \n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False]\n",
      " [False]\n",
      " [False]\n",
      " ..., \n",
      " [False]\n",
      " [False]\n",
      " [False]]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1546   49]\n",
      " [ 261  144]]\n"
     ]
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  84.5 %\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "accuracy = (tn+tp)/(tn + fp + fn + tp) * 100\n",
    "print(\"Accuracy: \",accuracy,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting a single new observation \n",
    "\n",
    "###### Predict if the below customer will exit the bank ?\n",
    "\n",
    "* Geography: France\n",
    "* Credit Score: 600\n",
    "* Gender: Male\n",
    "* Age: 40 years old\n",
    "* Tenure: 3 years\n",
    "* Balance: \\$60000\n",
    "* Number of Products: 2\n",
    "* Does this customer have a credit card ? Yes\n",
    "* Is this customer an Active Member: Yes\n",
    "* Estimated Salary: \\$50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False]]\n"
     ]
    }
   ],
   "source": [
    "customer = np.array([[0.0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])\n",
    "scaled_customer = sc.transform(customer)\n",
    "new_prediction = classifier.predict(scaled_customer)\n",
    "new_prediction = (new_prediction > 0.5)\n",
    "print(new_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating and Improving the performance of ANN\n",
    "\n",
    "We will be using K-Fold Validation to evaluate the performance of the ANN. We will be using Grid Search to tune the performance of the algorithm. We will also see how the model's performance will improve if we use dropouts. The execution output for the below parts are not shown as they are very large. I have included the final result obtained for each of the below sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units=6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
    "    classifier.add(Dense(units=6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KerasClassifier(build_fn = build_classifier, batch_size = 10, nb_epochs = 100)\n",
    "accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=10, n_jobs=-1)\n",
    "print(\"Accuracies: \",accuracies)\n",
    "print(\"Accuracies Mean: \",accuracies.mean())\n",
    "print(\"Accuracies SD: \",accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, the following results were obtained\n",
    "\n",
    "* Accuracies: \\[ 0.85374999 0.855,  0.87625     0.83374999  0.88249999  0.8525\n",
    "  0.83625     0.825       0.85124999  0.86 \\]\n",
    "* Accuracies Mean:  0.852624994963\n",
    "* Accuracies SD:  0.0170335144316"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Imporving the performance of the model using Dropouts\n",
    "Dropout regularization is used to reduce overfitting. In this, we randomly set a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(units=6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
    "classifier.add(Dropout(rate= 0.1))\n",
    "classifier.add(Dense(units=6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "classifier.add(Dropout(rate= 0.1))\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "accuracy = (tn+tp)/(tn + fp + fn + tp) * 100\n",
    "print(\"Accuracy: \",accuracy,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training with Dropout, the following accuracy was obtained.\n",
    "\n",
    "Accuracy:  85.35 %\n",
    "\n",
    "We can see that the performance of the network did improve with dropouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tunning the performance of the model using Grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def build_classifier(optimizer):\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units=6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
    "    classifier.add(Dense(units=6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier \n",
    "\n",
    "classifier = KerasClassifier(build_fn = build_classifier)\n",
    "parameters = {'batch_size': [25, 32], 'epochs': [100, 500], 'optimizer': ['adam', 'rmsprop']}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10)\n",
    "\n",
    "grid_search = grid_search.fit(X_train,y_train)\n",
    "\n",
    "best_accuracy = grid_search.best_score_\n",
    "print(\"Best Accuracy: \", best_accuracy)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters: \", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On execution, Grid Search provided the following outputs.\n",
    "\n",
    "Best Accuracy:  0.8516\n",
    "\n",
    "Best Parameters:  {'batch_size': 32, 'nb_epoch': 500, 'optimizer': 'rmsprop'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** When I trained the network using these parameters, the network was able to achieve an accuracy of 86.35 % **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
